---
title: Using Julia to compute the Kantorovich distance
date : 2014-04-09
--- &lead

```{r setup0, echo=FALSE}
opts_chunk$set(fig.path="assets/fig/KantorovichWithJulia", tidy=FALSE)
```


 In the article ['Using R to compute the Kantorovich distance'](http://stla.github.io/stlapblog/posts/KantorovichWithR.html) I have shown how to use the [cddlibb C library](http://web.mit.edu/sage/export/tmp/cddlib-094b/doc/cddlibman.pdf) through R with the help of the [rccd R package](http://cran.r-project.org/web/packages/rcdd/vignettes/vinny.pdf) to compute the Kantorovich distance between two probability measures (on a finite set). In the present article I show how to do so using [Julia](http://julialang.org/). 
 Similarly to the R way, the Julia way uses a C library, the [GLPK (GNU Linear Programming Kit)](http://www.gnu.org/software/glpk/) library, wrapped in a Julia package, named [GLPK](http://docs.julialang.org/en/release-0.1/stdlib/glpk/) too. 
 

## Data of the problem

First, we define the probability measures $\mu$ and $\nu$ between which we want the Kantorovich distance to be computed.

```{r, eval=FALSE}
mu = [1/7, 2/7, 4/7]
nu = [1/4, 1/4, 1/2]
```

Recall that the Kantorovich distance is defined from an initial distance which we take to be the $0-1$ distance, and is represented in the $D$ matrix defined as follows with Julia:

```{r, eval=FALSE}
n = length(mu)
D = 1-eye(n);
```

```{r, eval=FALSE}
julia> D
3x3 Array{Float64,2}:
 0.0  1.0  1.0
 1.0  0.0  1.0
 1.0  1.0  0.0
```

Thus, the Julia `eye` function is similar to the R `diag` function. 

## Constraint matrix

The constraints matrix is 
$$
A=\begin{pmatrix} M_1 \\ M_2 \end{pmatrix} 
$$
where 
$$ M_1 = \begin{pmatrix}
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 & 1 & 1  & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1  
\end{pmatrix} $$
defines the linear equality constraints corresponding to $\mu$ 
and 
$$ M_2 = \begin{pmatrix}
1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
 0 & 1 & 0 & 0 & 1 & 0  & 0 & 1 & 0 \\
 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1  
\end{pmatrix}. $$
defines the linear equality constraints corresponding to $\nu$. 

I define these matrices as follows in Julia:

```{r, eval=FALSE}
M1=zeros(n,n*n)
for i in 1:n
        M1[i,(1:n)+n*(i-1)]=1
end
M2=repmat(eye(n),1,n)
A=vcat(M1,M2);
```

```{r, eval=FALSE}
julia> A
6x9 Array{Float64,2}:
 1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0
 1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0
 0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0
 0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0
```

In order to process the constraints matrix $A$ with `GLPK` we have to represent it using three vectors `ia`, `ja`, and `ar`, respectively giving the index of the row, the index of the columns, and the value of the corresponding entry of $A$:

```{r, eval=FALSE}
ia = zeros(Int32,length(A));
ja = zeros(Int32,length(A));
ar = zeros(Float64,length(A));
k=0
for i in 1:size(A,1)
    for j in 1:size(A,2)
        k=k+1
        ia[k] = i
        ja[k] = j
        ar[k] = A[i,j]
    end
end
```

Recall that the constraints are the linear equality constraints 
$$M_1 P = \begin{pmatrix} \mu(a_1) \\ \mu(a_2) \\ \mu(a_3) \end{pmatrix} 
\qquad \text{and} \qquad 
M_2 P = \begin{pmatrix} \nu(a_1) \\ \nu(a_2) \\ \nu(a_3) \end{pmatrix}$$ 
where $P$ is the vector formed by the variables $p_{ij} \geq 0$.

## GLPK in action 

First load the package, initialize the problem, and give it a name:

```{r, eval=FALSE}
using GLPK 
lp = GLPK.Prob()
GLPK.set_prob_name(lp, "kanto")
```

Computing the Kantorovich distance is a minimization problem, declared as follows:

```{r, eval=FALSE}
GLPK.set_obj_dir(lp, GLPK.MIN)
```

(`obj` refers to *objective function*, the function to be optimized).

Now we use the `GLPK.set_row_bnds` function to  set the hand side vector (the *bounds*) 
of the linear constraints and to specify the type of 
our constraints. Here these are *equality* constraints and this type is specified by `GLPK.FX`:

```{r, eval=FALSE}
GLPK.add_rows(lp, size(A,1))
for i in 1:n
    GLPK.set_row_bnds(lp, i, GLPK.FX, mu[i], mu[i])
    GLPK.set_row_bnds(lp, i+n, GLPK.FX, nu[i], nu[i])
end
```

Now we specify the positivy constraints $p_{ij} \geq 0$ about the variables $p_{ij}$ corresponding to the columns of the constraint matrix:

```{r, eval=FALSE}
GLPK.add_cols(lp, size(A,2))
k=0
for i in 1:n
    for j in 1:n
        k = k+1
        GLPK.set_col_bnds(lp, k, GLPK.LO, 0.0, 0.0)
        GLPK.set_obj_coef(lp, k, D[i,j])
    end
end
```

We are ready ! Load the matrix, run the algorithm :

```{r, eval=FALSE}
GLPK.load_matrix(lp, ia, ja, ar)
GLPK.simplex(lp);
```

and get the solution; 

```{r, eval=FALSE}
julia> GLPK.get_obj_val(lp)
0.10714285714285715
```


## How to get the exact result ?

As we have seen in the [previous article](http://stla.github.io/stlapblog/posts/KantorovichWithR.html), the exact Kantorovich distance between $\mu$ and $\nu$ is $\dfrac{3}{28}$:

```{r, eval=FALSE}
> 3/28
[1] 0.1071429
```

There's a function `GLPK.exact` in the Julia `GLPK` package allowing to get this exact result, but currently I have not been able to make it work. 

