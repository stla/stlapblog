<!DOCTYPE html>
<html>
<head>
  <title>Sample size determination for a Gaussian mean</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="" />
  <meta name="author" content="">
  <link rel="shortcut icon" href="assets/img/07-10-06_2241.jpg">
  <link rel="alternate" type="application/rss+xml" href="">
  <link href="../libraries/frameworks/purus/css/bootstrap.min.css" rel="stylesheet" />
  <link href="../libraries/frameworks/purus/css/bootstrap-responsive.min.css" rel="stylesheet" />
  <link href="../libraries/frameworks/purus/css/main.css" rel="stylesheet" />
  <link href="../libraries/highlighters/prettify/css/twitter-bootstrap.css" rel="stylesheet">
  <!-- IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link href='http://fonts.googleapis.com/css?family=Raleway:400,600,200,800' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Droid+Sans' rel='stylesheet' type='text/css'>
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
    <style>
  	#disqus_thread {
		margin-top: 140px;
	}
	  #sidebar .sidebar-nav .info h3 a:hover, a:hover { color: #01A9DB; }
	  #sidebar .sidebar-nav #avatar img, #sidebar .sidebar-nav ul#links li.active a { border-color: #01A9DB; }
	  #sidebar .sidebar-nav ul#links li a:hover { background: #01A9DB; }
    p {text-align: justify;}
  </style>
  <link rel="stylesheet" href = "../assets/css/box_with_title.css">
<link rel="stylesheet" href = "../assets/css/custom.css">
<link rel="stylesheet" href = "../assets/css/ribbons.css">

</head>
<body>
	<div class="container-fluid">
		<div class="row-fluid">
			<div id="sidebar" class="span2">
			  <div class="sidebar-nav sidebar-nav-fixed">
				  <div class="info">
				    <p id="avatar"><a href="#"><img alt="Title" src="http://stla.github.com/stlapblog/assets/img/07-10-06_2241.jpg " style="width:250px" /></a></p>
				    <h3><a href="/">stlaPblog </a></h3>
					  <p class="description">a blog about Mathematics, R, Statistics, ...</p>
					</div>
					<ul id="links">
			        <li><a href="http://stla.github.com/stlapblog/index.html">Home</a></li>
        <li><a href="http://stla.github.com/stlapblog/about.html">About</a></li>
      
				  <br/>
				  <div style="padding-left: 5px;">
					<h3>Some links</h3>
					<h4>&#9654 R links</h4>
						<div style="padding-left: 5px;">
					    <ul>
					      <li><a href="http://github.com/ramnathv/poirot/"><u>Poirot</u></a>Reproducible Blogging with R Markdown</li>
			          <li><a href="http://slidify.org/"><u>Slidify</u></a>Reproducible html5 slides from R markdown</li>
			          <li><a href="http://www.r-bloggers.com/"><u>R-bloggers</u></a>Blog posts about R, contributed by R bloggers worldwide.</li>
					    </ul>
				    </div>
			    <br/>
			  	<h4>&#9654 Blogs</h4>
						<div style="padding-left: 5px;">
					    <ul>
					      <li><a href="http://stla.overblog.com/"><u>stla.overblog</u></a>My previous blog</li>
			          <li><a href="http://timelyportfolio.blogspot.be/"><u>Timely Portfolio</u>
			</a>A great blog about R, Javascript, and more</li>
					    </ul>
				    </div>
					</div>
			    </ul>
				</div>
			</div>
						<div id="content" class='span10'>
				<div id="post-wrapper">
          <ol id="posts">
            <li class="post">
              <h3>
                <a href="#">Sample size determination for a Gaussian mean</a>
              </h3>
              <span>2013-04-13</span><br/>
               <a class='label label-success' href='https://raw.github.com/stla/stlapblog/gh-pages/posts/samplesize_mean.Rmd'>Source</a>
              <div class='lead'>
<p>This article explains the methodology implemented in the Shiny application available at <a href="http://glimmer.rstudio.com/stla/samplesize_mean/">http://glimmer.rstudio.com/stla/samplesize_mean/</a></p>

<h2>Statement of the problem </h2>

<p>Consider a preliminary experiment \({\cal E}_0\), whose issue is a sample \(y_0\) of size \(n_0=5\) generated from a normal distribution with unknown mean \(\mu\) and standard deviation \(\sigma\). This expriment is simulated below (I voluntary hide the unknown values of the parameters!)</p>

<pre><code class="r">n0 &lt;- 5
(y0 &lt;- rnorm(n0, mu, sigma))
</code></pre>

<pre><code>## [1] 10.753 12.014  9.645 12.028  7.783
</code></pre>

<p>Now you want to plan a new experiment \({\cal E}^{\text{new}}\) under the same conditions as \({\cal E}_0\), after which you hope to get a reliable estimate of 
the mean \(\mu\). 
For instance you wonder <em>how to optimally chose the sample size \(n\) of \({\cal E}^{\text{new}}\) in order to have \(80\%\) probability to the half-width of the \(95\%\)-confidence interval about \(\mu\) being less than \(2\) ?</em> </p>

<h2>A naive solution </h2>

<p>At the issue of the future experiment \({\cal E}^{\text{new}}\), 
the half-width of the \(95\%\)-confidence interval about the mean is 
\(\boxed{t^*_{n-1}(2.5\%)\dfrac{sd(y^{\text{new}})}{\sqrt{n}}}\), denoting by 
\(y^{\text{new}}\) the new generated sample of size \(n\). </p>

<p>Let&#39;s take a look at the estimated standard deviation of the preliminary sample:</p>

<pre><code class="r">(sd0 &lt;- sd(y0))
</code></pre>

<pre><code>## [1] 1.788
</code></pre>

<p>Then we can plot <em>the half-width that  would be obtained if the new sample 
standard deviation \(sd(y^{\text{new}}\)) would exactly equal to the 
sample standard deviation \(sd(y_0)\) of the preliminary sample</em>:</p>

<pre><code class="r">hwidth &lt;- function(n, sd, alpha = 5/100) {
    qt(1 - alpha/2, n - 1) * sd/sqrt(n)
}
n.range &lt;- 3:30
names(n.range) &lt;- paste0(&quot;n=&quot;, n.range)
library(ggplot2)
dfplot &lt;- data.frame(n = n.range, hw = hwidth(n.range, sd0))
p &lt;- ggplot(data = dfplot, aes(x = n, y = hw)) + geom_line(colour = &quot;blue&quot;) + 
    geom_point(colour = &quot;brown&quot;) + xlab(&quot;sample size&quot;) + ylab(&quot;half-width&quot;)
print(p)
</code></pre>

<p><img src="assets/fig/sampleSizeGaussianMeanunnamed-chunk-3.png" alt="plot of chunk unnamed-chunk-3"> </p>

<h2>Taking into account the uncertainty about the preliminary estimate</h2>

<p>Of course we cannot expect that the future sample standard deviation will be exactly equal to the preliminary one;  basing our choice of the sample size on the blue line would not take account of <em>risk</em>.</p>

<h3>Wrong approach: Using a confidence interval about the theoretical standard deviation</h3>

<p>It is well known that 
\[
sd^2(y_0) \sim \sigma^2\frac{\chi^2_{n_0-1}}{n_0-1} = \sigma^2F_{n_0-1,\infty}.
\]
Consequently, an upper \(80\%\)-confidence bound about the unknown standard deviation \(\sigma\) 
can be derived from the preliminary experiment \({\cal E}_0\) as follows. 
Let \(q\) be the lower \(20\%\)-quantile of the
chi-squared distribution \(\chi^2_{n_0-1}\), defined by 
\(\Pr(\chi^2_{n_0-1} > q)=80\%\). Then the interval
\[
\left[0, \frac{\sqrt{n_0-1}sd(y_0)}{\sqrt{q}} \right] 
\]
is the left-sided \(80\%\)-confidence interval about \(\sigma\) derived from 
the preliminary sample. In other words, 
\(\boxed{\sigma^+=  \dfrac{\sqrt{n_0-1}sd(y_0)}{\sqrt{q}}}\) is an upper \(80\%\)-confidence
bound for \(\sigma\). </p>

<p>Note that this fact has a Bayesian counterpart. 
Considering the <a href="http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf">noninformative reference prior</a>, 
the posterior distribution of \(1/\sigma^2\) is the Gamma distribution 
\({\cal G}\left(\frac{n_0-1}{2},\frac{(n_0-1)sd^2(y_0)}{2}\right)\). 
Then the \(80\%\)-confidence interval given above is also the left-sided 
\(80\%\)-credibility interval arising from the posterior distribution of 
\(\sigma\) induced by the posterior Gamma distribution of \(1/\sigma^2\).</p>

<p>Now, in addition to the blue line 
\(n \mapsto t^*_{n-1}(2.5\%)\dfrac{sd(y_0)}{\sqrt{n}}\) plotted above, let&#39;s plot the <em>upper \(80\%\)-confidence line</em> 
\(n \mapsto t^*_{n-1}(2.5\%)\dfrac{\sigma^+}{\sqrt{n}}\):  </p>

<pre><code class="r">upperbound &lt;- function(n, sd, level = 80/100) {
    sqrt(n - 1) * sd/sqrt(qchisq(1 - level, n - 1))
}
(sigma.upper &lt;- upperbound(n0, sd0))
</code></pre>

<pre><code>## [1] 2.785
</code></pre>

<pre><code class="r">dfplot$hw.upper &lt;- hwidth(n.range, sigma.upper)
p &lt;- p + geom_line(data = dfplot, aes(x = n, y = hw.upper), colour = &quot;red&quot;, 
    linetype = &quot;dashed&quot;)
print(p)
</code></pre>

<p><img src="assets/fig/sampleSizeGaussianMeanunnamed-chunk-4.png" alt="plot of chunk unnamed-chunk-4"> </p>

<p>The upper confidence bound is approximately 2.14 for \(n=9\) and approximately 1.99 for \(n=10\).  </p>

<p>Thus, would you consider taking \(n=10\) as a \(80\%\)-guarantee that the half-width 
of your confidence interval will be less than \(2\) ?
<strong>This approach is conceptually wrong</strong>:</p>

<ul>
<li><p>\(t^*_{n-1}(2.5\%)\dfrac{\sigma^+}{\sqrt{n}}\) is an upper \(80\%\)-confidence bound for the &quot;theoretical half-width&quot; \(t^*_{n-1}(2.5\%)\dfrac{\sigma}{\sqrt{n}}\) (note this is \(\sigma\) in the numerator of the fraction), which does not really makes sense;</p></li>
<li><p>whereas you want \(80\%\)-probability for the <em>observed</em> half-width \(t^*_{n-1}(2.5\%)\dfrac{sd(y^{\text{new}})}{\sqrt{n}}\) being less than \(2\).</p></li>
</ul>

<h3>Correct approach: using a prediction interval about the sample standard deviation</h3>

<p>Let&#39;s firstly consider the Bayesian approach. After some elementary calculations, it is easy to write down the density function of the posterior distribution of \(\sigma\) with the help of the Gamma density function:</p>

<pre><code class="r"># firstly define the density function of the inverse square root of
# Gamma(a,b)
dsqrtigamma &lt;- function(x, a, b) {
    dgamma(1/x^2, a, b) * 2/x^3
}
# posterior density of sigma
dsigma &lt;- function(x, n, sd) {
    dsqrtigamma(x, (n - 1)/2, (n - 1) * sd^2/2)
}
</code></pre>

<p>We put the code plotting this distribution in a function in order to call  the same plot later:</p>

<pre><code class="r">fplot &lt;- function(from = 0, to = 6) {
    curve(dsigma(x, n0, sd0), from = from, to = to, xlab = expression(sigma), 
        ylab = NA, axes = FALSE, main = expression(paste(&quot;Posterior distribution of &quot;, 
            sigma)))
    axis(1)
    abline(v = sd0, lty = &quot;dotted&quot;, col = &quot;grey50&quot;)
}
fplot()
</code></pre>

<p><img src="assets/fig/sampleSizeGaussianMeanunnamed-chunk-6.png" alt="plot of chunk unnamed-chunk-6"> </p>

<p>This distribution describes the uncertainty about the unknown theoretical standard deviation \(\sigma\) after the preliminary experiment \({\cal E}_0\) was performed, whereas uncertainty about the future sample standard deviation \(sd(y^{\text{new}})\) is described by its <em>posterior predicitve distribution</em>. 
The posterior predictive distribution of \(sd^2(y^{\text{new}})\) is obtained by averaging its distribution  conditionally to a given value of \(\sigma\):
\[
\left(sd^2(y^{\text{new}}) \mid \sigma\right) \sim \sigma^2\frac{\chi^2_{n-1}}{n-1}.
\]
over the posterior distribution of \(\sigma\) which is given by 
\[\frac{1}{\sigma^2} \sim {\cal G}\left(\frac{n_0-1}{2},\frac{(n_0-1)sd^2(y_0)}{2}\right).\]</p>

<h3>Simulated posterior predictive distribution</h3>

<p>Thus the posterior predictive distribution of \(sd(y^{\text{new}})\) is very easy to simulate:</p>

<pre><code class="r">nsims &lt;- 12
sdnew.sims &lt;- array(NA, dim = c(nsims, length(n.range)))
colnames(sdnew.sims) &lt;- names(n.range)
for (i in 1:nsims) {
    # sample from the posterior distribution of sigma :
    inv.sigma2 &lt;- rgamma(1, (n0 - 1)/2, sd0^2 * (n0 - 1)/2)
    for (j in 1:length(n.range)) {
        sdnew.sims[i, j] &lt;- sqrt(1/inv.sigma2 * rchisq(1, n.range[j] - 1)/(n.range[j] - 
            1))
    }
}
par(lwd = 2)
fplot()
j &lt;- 8
lines(density(sdnew.sims[, j], from = 0), col = &quot;red&quot;)
legend1 &lt;- paste0(&quot;predictive sd with n=&quot;, n.range[j])
j &lt;- 18
lines(density(sdnew.sims[, j], from = 0), col = &quot;blue&quot;)
legend2 &lt;- paste0(&quot;predictive sd with n=&quot;, n.range[j])
j &lt;- 28
lines(density(sdnew.sims[, j], from = 0), col = &quot;green&quot;)
legend3 &lt;- paste0(&quot;predictive sd with n=&quot;, n.range[j])
legend(&quot;topright&quot;, legend = c(&quot;posterior sigma&quot;, legend1, legend2, legend3), 
    text.col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), lty = &quot;solid&quot;, col = c(&quot;black&quot;, 
        &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;))
</code></pre>

<p><img src="assets/fig/sampleSizeGaussianMeanunnamed-chunk-7.png" alt="plot of chunk unnamed-chunk-7"> </p>

<p>And now we could be able to plot the <em>upper \(80\%\)-prediction line</em> \(n \mapsto t^*_{n-1}(2.5\%)\dfrac{sd(y^{\text{new}})^+}{\sqrt{n}}\) because we can estimate the upper \(80\%\)-prediction bound \(sd(y^{\text{new}})^+\) of \(sd(y^{\text{new}})\) (depending on \(n\)):</p>

<pre><code class="r">apply(sdnew.sims, 2, FUN = function(x) quantile(x, probs = 80/100))
</code></pre>

<pre><code>##   n=3   n=4   n=5   n=6   n=7   n=8   n=9  n=10  n=11  n=12  n=13  n=14 
## 3.383 3.289 3.741 3.573 2.790 2.832 3.676 3.480 3.646 3.160 3.452 3.483 
##  n=15  n=16  n=17  n=18  n=19  n=20  n=21  n=22  n=23  n=24  n=25  n=26 
## 3.978 3.520 3.867 3.486 3.526 3.151 3.647 3.338 3.218 3.247 2.650 3.609 
##  n=27  n=28  n=29  n=30 
## 3.678 3.510 3.052 3.230
</code></pre>

<p>But wait, instead of using simulations, we will use exact calculations.</p>

<h3>Theoretical posterior predictive distribution</h3>

<p>Let&#39;s have a theoretical look at the posterior predictive distribution of the future sample standard deviation. Firstly recall that 
\[
\left(sd^2(y^{\text{new}}) \mid \sigma\right) \sim \sigma^2\frac{\chi^2_{n-1}}{n-1}.
\]
Then recall the posterior distribution 
\[\frac{1}{\sigma^2} \sim {\cal G}\left(\frac{n_0-1}{2},\frac{(n_0-1)sd^2(y_0)}{2}\right)=
\frac{1}{(n_0-1)sd^2(y_0)}\chi^2_{n_0-1},\]
hence the posterior predictive distribution of \(sd^2(y^{\text{new}})\) is
(where the \(\chi^2\) variates in the fraction are independent):
\[
sd^2(y^{\text{new}}) \sim 
sd^2(y_0) \frac{n_0-1}{n-1}\frac{\chi^2_{n-1}}{\chi^2_{n_0-1}}
=sd^2(y_0)F(n-1, n_0-1).\]</p>

<p>So let&#39;s define the posterior predictive density of \(sd(y^{\text{new}})\) in R:</p>

<pre><code class="r"># firstly define the density of the square root of Fisher distribution:
dsqrtF &lt;- function(x, ...) {
    df(x^2, ...) * 2 * x
}
# now define the posterior predicitve of sd(ynew):
dsdnew &lt;- function(x, n0, sd0, n) {
    scale &lt;- sd0
    dsqrtF(x/scale, n - 1, n0 - 1)/scale
}
</code></pre>

<p>And let&#39;s compare with the red density in the previous figure: </p>

<pre><code class="r">curve(dsdnew(x, n0, sd0, n = 20), from = 0, to = 6, xlab = expression(sigma), 
    ylab = NA)
abline(v = sd0, lty = &quot;dotted&quot;, col = &quot;grey50&quot;)
lines(density(sdnew.sims[, 18], from = 0), col = &quot;red&quot;)
</code></pre>

<p><img src="assets/fig/sampleSizeGaussianMeanunnamed-chunk-10.png" alt="plot of chunk unnamed-chunk-10"> </p>

<p>Very nice. We don&#39;t need simulations anymore because we are also able to define the inverse cumulative distribution function of the posterior predictive distribution of \(sd(y^{\text{new}})\):</p>

<pre><code class="r"># firstly define the density of the square root of Fisher distribution:
qsqrtF &lt;- function(p, ...) {
    sqrt(qf(p, ...))
}
# now define the posterior predicitve of sd(ynew):
qsdnew &lt;- function(x, n0, sd0, n) {
    scale &lt;- sd0
    scale * qsqrtF(x, n - 1, n0 - 1)
}
</code></pre>

<p>Thus the Bayesian upper \(80\%\)-prediction bound of \(sd(y^{\text{new}})\) is 
\[
sd(y^{\text{new}})^+=
sd(y_0) \times \sqrt{F_{n-1,n_0-1}(0.8)}. 
\]
So now in addition to the blue line 
\(n \mapsto t^*_{n-1}(2.5\%)\dfrac{sd(y_0)}{\sqrt{n}}\) of our first figure, we can plot the <em>upper \(80\%\)-prediction line</em> \(n \mapsto t^*_{n-1}(2.5\%)\dfrac{sd(y^{\text{new}})^+}{\sqrt{n}}\): </p>

<pre><code class="r">(sdnew.upper &lt;- qsdnew(80/100, n0, sd0, n.range))
</code></pre>

<pre><code>##   n=3   n=4   n=5   n=6   n=7   n=8   n=9  n=10  n=11  n=12  n=13  n=14 
## 2.812 2.819 2.817 2.815 2.812 2.810 2.808 2.806 2.804 2.803 2.802 2.801 
##  n=15  n=16  n=17  n=18  n=19  n=20  n=21  n=22  n=23  n=24  n=25  n=26 
## 2.800 2.799 2.798 2.798 2.797 2.797 2.796 2.796 2.795 2.795 2.794 2.794 
##  n=27  n=28  n=29  n=30 
## 2.794 2.793 2.793 2.793
</code></pre>

<pre><code class="r">dfplot$hw.upper.pred &lt;- hwidth(n.range, sdnew.upper)
p &lt;- p + geom_line(data = dfplot, aes(x = n, y = hw.upper.pred), colour = &quot;green&quot;, 
    linetype = &quot;dashed&quot;)
print(p)
</code></pre>

<p><img src="assets/fig/sampleSizeGaussianMeanunnamed-chunk-12.png" alt="plot of chunk unnamed-chunk-12"> </p>

<p>Well, I agree... no large difference with the confidence line ! 
This is not surprising: we have just seen that
\[
sd(y^{\text{new}})^+=
sd(y_0) \times \sqrt{F_{n-1,n_0-1}(0.8)}
\]
and in fact the upper \(80\%\)-confidence bound of \(\sigma\) can  be written as
\[
\sigma^+=
sd(y_0) \times \sqrt{F_{\infty,n_0-1}(0.8)}.
\]
But I cannot guarantee that the two lines will always be as close in every possible situations.</p>

<h3>Back to the frequentist framework</h3>

<p>We have seen that 
\[
sd(y^{\text{new}})^+=
sd(y_0) \times \sqrt{F_{n-1,n_0-2}(0.8)}. 
\]
is the Bayesian upper \(80\%\)-prediction bound of the future sample standard deviation \(sd(y^{\text{new}})\) (derived from the noninformative reference prior).
Actually it is a frequentist  \(80\%\)-prediction bound too, as a consequence of the obvious distributional equality
\[
\frac{sd^2(y^{\text{new}})}
{sd^2(y_0)} \sim F_{n-1,n_0-1}. 
\]
When the size \(n\) of the future sample goes to \(\infty\), 
\[
\frac{sd^2(y^{\text{new}})}
{sd^2(y_0)} \sim F_{\infty,n_0-1} = \frac{n_0-1}{\chi^2_{n_0-1}}, 
\]
and the asymptotic \(80\%\)-prediction bound of \(sd(y^{\text{new}})\) is nothing but the \(80\%\)-confidence bound of \(\sigma\). </p>

<p>That shows that, if we run the following algorithm, then we are sure that the \(80\%\)-prediction approach is successful in at least  \(80\%\) of simulations, 
but the \(80\%\)-confidence approach is only asymptotically (\(n\to\infty\)) successful in at least  \(80\%\) of simulations,</p>

<p><strong>Algorithm:</strong>  </p>

<ol>
<li><p>Simulate \(n_0=5\) values from the Gaussian distribution \({\cal N}(\mu,\sigma^2)\) with fixed \(\mu\) and \(\sigma\)</p></li>
<li><p>Derive the optimal sample size \(n\) required to expect a half-width smaller than \(2\)</p>

<ul>
<li>using the \(80\%\)-confidence line approach</li>
<li>or using the \(80\%\)-prediction line approach</li>
</ul></li>
<li><p>Simulate \(n\) values from the Gaussian distribution \({\cal N}(\mu,\sigma^2)\) and say the approach is successful if the  half-width is smaller than \(2\).</p></li>
</ol>

<h2>Note on the Bayesian approach</h2>

<p>We have seen a Bayesian approach and a frequentist approach that lead both the same result. But besides its better interpretability, the Bayesian approache has another advantage. The Bayesian way to get a prediction interval about the sample standard deviation is conceptually straightforward: we just have to derive the posterior predictive distribution by doing direct calculations. On the other hand, generally speaking, the frequentist approach requires that we find a &quot;trick&quot; to derive a prediction interval. Of course here this is a simple example and it was not difficult to think of the distributional equality
\(\frac{sd^2(y^{\text{new}})}{sd^2(y_0)} \sim F_{n-1,n_0-1}\). But I mean that, unlike the Bayesian approach, the frequentist approach is not driven by a general method not depending on the particular statistical model of interest. </p>

</div>
              <div id="disqus_thread"></div>
            </li>
          </ol>
          <div class="pagination">
            <ul>
              <li><a href="http://stla.github.com/stlapblog/">&#171; Back Home</a></li>
            </ul>
          </div> 
        </div>
			</div>
		</div>
  </div>
</body>
  <script src='../libraries/frameworks/purus/js/bootstrap.min.js'></script>
  <script>
      var disqus_developer = 1;
      var disqus_shortname = 'stlapblog'; 
      // required: replace example with your forum shortname
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); 
          dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || 
           document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- Google Prettify -->
  <script src="http://cdnjs.cloudflare.com/ajax/libs/prettify/188.0.0/prettify.js"></script>
  <script src='../libraries/highlighters/prettify/js/lang-r.js'></script>
  <script>
    var pres = document.getElementsByTagName("pre");
    for (var i=0; i < pres.length; ++i) {
      pres[i].className = "prettyprint linenums";
    }
    prettyPrint();
  </script>
  <!-- End Google Prettify --> 
  </html>